{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1wKhIH7G2h45o8ZI-E8d0llGUvhkl-I5q",
      "authorship_tag": "ABX9TyNPx6R6WIENyg/EHca7dV22",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jimboH/CS224n/blob/master/T_patcher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8WDLNikARDs",
        "outputId": "5cf75de7-2db2-4e71-8690-5616475f590d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Transformer-Patcher'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 43 (delta 6), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (43/43), 86.29 KiB | 910.00 KiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ZeroYuHuang/Transformer-Patcher.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('Transformer-Patcher')"
      ],
      "metadata": {
        "id": "9KpRKM71ApMr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyH7R_ASArjK",
        "outputId": "b4ac0e86-4f26-4853-9aaf-c4eafc3e661f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.1.0+cu121)\n",
            "Requirement already satisfied: pytorch_lightning==1.8.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.8.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.37.2)\n",
            "Collecting torchmetrics==0.9.1 (from -r requirements.txt (line 4))\n",
            "  Downloading torchmetrics-0.9.1-py3-none-any.whl (419 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.7/419.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.25.2)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.8.4->-r requirements.txt (line 2)) (4.66.2)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.8.4->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.8.4->-r requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: tensorboardX>=2.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.8.4->-r requirements.txt (line 2)) (2.6.2.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.8.4->-r requirements.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.8.4->-r requirements.txt (line 2)) (4.9.0)\n",
            "Requirement already satisfied: lightning-utilities!=0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.8.4->-r requirements.txt (line 2)) (0.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 3)) (0.20.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 3)) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 3)) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 3)) (0.4.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->-r requirements.txt (line 5)) (23.2.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning==1.8.4->-r requirements.txt (line 2)) (3.9.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities!=0.4.0,>=0.3.0->pytorch_lightning==1.8.4->-r requirements.txt (line 2)) (67.7.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX>=2.2->pytorch_lightning==1.8.4->-r requirements.txt (line 2)) (3.20.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.8.4->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.8.4->-r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.8.4->-r requirements.txt (line 2)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.8.4->-r requirements.txt (line 2)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.8.4->-r requirements.txt (line 2)) (4.0.3)\n",
            "Installing collected packages: torchmetrics\n",
            "  Attempting uninstall: torchmetrics\n",
            "    Found existing installation: torchmetrics 1.3.1\n",
            "    Uninstalling torchmetrics-1.3.1:\n",
            "      Successfully uninstalled torchmetrics-1.3.1\n",
            "Successfully installed torchmetrics-0.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/dataset/fever_dataloader.py\n",
        "!python src/dataset/zsre_dataloader.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ljxko2BB5Rk",
        "outputId": "05162cb3-4804-4ef5-b0f2-e18204ef3a14"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading all data\n",
            "Splitting data into three parts according the ratios\n",
            "The dataset has been shuffled and the first 5 item of toy list is [75549, 32525, 22087, 80984, 3846]\n",
            "For train data, we got 83972 data points\n",
            "For val data, we got 10496 data points\n",
            "For edit data, we got 10496 data points\n",
            "Loading data\n",
            "Loading test data\n",
            "Splitting the existing data according to the ratios\n",
            "For train data, we got 219755 data points\n",
            "For val data, we got 18312 data points\n",
            "For edit data, we got 6104 data points\n",
            "For test data, we got 27644 data points\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ../drive/MyDrive/research/T-patcher/data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKsIhTFeCXVO",
        "outputId": "030c0e1d-7c10-478a-bc89-17f45571fec2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ../drive/MyDrive/research/T-patcher/data.zip\n",
            "   creating: data/fever_data/\n",
            "  inflating: data/fever_data/fever-dev-kilt.jsonl  \n",
            "  inflating: data/fever_data/fever-train-kilt.jsonl  \n",
            "   creating: data/zsre_data/\n",
            "  inflating: data/zsre_data/structured_zeroshot-dev-new_annotated_final.jsonl  \n",
            "  inflating: data/zsre_data/structured_zeroshot-train-new_annotated_final.jsonl  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/train_bart_seq2seq.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg1JKVxeDg6y",
        "outputId": "97c5f714-9521-418a-c795-4043ef5e126e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/seed.py:48: LightningDeprecationWarning: `pytorch_lightning.utilities.seed.seed_everything` has been deprecated in v1.8.0 and will be removed in v1.10.0. Please use `lightning_lite.utilities.seed.seed_everything` instead.\n",
            "  rank_zero_deprecation(\n",
            "Global seed set to 42\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:441: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
            "  rank_zero_deprecation(\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "The cache can not be used\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type                         | Params\n",
            "-----------------------------------------------------------\n",
            "0 | model     | BartForConditionalGeneration | 139 M \n",
            "1 | train_acc | Accuracy                     | 0     \n",
            "2 | valid_acc | Accuracy                     | 0     \n",
            "-----------------------------------------------------------\n",
            "139 M     Trainable params\n",
            "0         Non-trainable params\n",
            "139 M     Total params\n",
            "557.682   Total estimated model params size (MB)\n",
            "Sanity Checking: 0it [00:00, ?it/s]/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "Sanity Checking DataLoader 0:   0% 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:433: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "Epoch 0:  86% 5980/6991 [17:03<02:53,  5.84it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1000 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  86% 6000/6991 [17:06<02:49,  5.85it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  86% 6020/6991 [17:08<02:45,  5.85it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  86% 6040/6991 [17:10<02:42,  5.86it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  87% 6060/6991 [17:12<02:38,  5.87it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  87% 6080/6991 [17:15<02:35,  5.87it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  87% 6100/6991 [17:17<02:31,  5.88it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  88% 6120/6991 [17:19<02:27,  5.89it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  88% 6140/6991 [17:21<02:24,  5.90it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  88% 6160/6991 [17:23<02:20,  5.90it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  88% 6180/6991 [17:25<02:17,  5.91it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  89% 6200/6991 [17:27<02:13,  5.92it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  89% 6220/6991 [17:29<02:10,  5.92it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  89% 6240/6991 [17:32<02:06,  5.93it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  90% 6260/6991 [17:34<02:03,  5.94it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  90% 6280/6991 [17:36<01:59,  5.94it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  90% 6300/6991 [17:38<01:56,  5.95it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  90% 6320/6991 [17:40<01:52,  5.96it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  91% 6340/6991 [17:43<01:49,  5.96it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  91% 6360/6991 [17:45<01:45,  5.97it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  91% 6380/6991 [17:47<01:42,  5.98it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  92% 6400/6991 [17:49<01:38,  5.98it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  92% 6420/6991 [17:51<01:35,  5.99it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  92% 6440/6991 [17:53<01:31,  6.00it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  92% 6460/6991 [17:56<01:28,  6.00it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  93% 6480/6991 [17:58<01:25,  6.01it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  93% 6500/6991 [18:00<01:21,  6.02it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  93% 6520/6991 [18:02<01:18,  6.02it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  94% 6540/6991 [18:04<01:14,  6.03it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  94% 6560/6991 [18:06<01:11,  6.04it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  94% 6580/6991 [18:08<01:08,  6.04it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  94% 6600/6991 [18:10<01:04,  6.05it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  95% 6620/6991 [18:12<01:01,  6.06it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  95% 6640/6991 [18:14<00:57,  6.06it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  95% 6660/6991 [18:17<00:54,  6.07it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  96% 6680/6991 [18:18<00:51,  6.08it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  96% 6700/6991 [18:21<00:47,  6.08it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  96% 6720/6991 [18:23<00:44,  6.09it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  96% 6740/6991 [18:25<00:41,  6.10it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  97% 6760/6991 [18:27<00:37,  6.10it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  97% 6780/6991 [18:29<00:34,  6.11it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  97% 6800/6991 [18:31<00:31,  6.12it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  98% 6820/6991 [18:34<00:27,  6.12it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  98% 6840/6991 [18:36<00:24,  6.13it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  98% 6860/6991 [18:38<00:21,  6.13it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  98% 6880/6991 [18:40<00:18,  6.14it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  99% 6900/6991 [18:42<00:14,  6.15it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  99% 6920/6991 [18:44<00:11,  6.15it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0:  99% 6940/6991 [18:47<00:08,  6.16it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0: 100% 6960/6991 [18:49<00:05,  6.16it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0: 100% 6980/6991 [18:51<00:01,  6.17it/s, loss=2.86, v_num=4, nll_loss=1.620]\n",
            "Epoch 0: 100% 6991/6991 [18:52<00:00,  6.17it/s, loss=2.93, v_num=4, nll_loss=1.830, valid_acc=0.208]\n",
            "Epoch 0: 100% 6991/6991 [18:52<00:00,  6.17it/s, loss=2.93, v_num=4, nll_loss=1.830, valid_acc=0.208]`Trainer.fit` stopped: `max_epochs=1` reached.\n",
            "Epoch 0: 100% 6991/6991 [18:53<00:00,  6.17it/s, loss=2.93, v_num=4, nll_loss=1.830, valid_acc=0.208]\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "model.safetensors: 100% 558M/558M [00:50<00:00, 11.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#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\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "rOK6z84R37MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/main.py --task=\"zsre\" --edit_folder_num=20 --process_folders=[0] --model_path=\"log/models/bart_seq2seq/version_4/checkpoints/test.ckpt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztTrdIEzJm22",
        "outputId": "8bb22acf-eb70-4851-fa9e-6715ad56a896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-03-01 04:28:01,590 - __main__ - INFO - The fold_to_gpu dict is: {0: 0}\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - All hyper-parameters are as follws\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - task:zsre\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - method:T-patch\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - edit_folder_num:20\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - process_folders:[0]\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - task_id:zsre_T-patch_20folders\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - gpu_nums:8\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - tasks_per_gpu:2\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - log_path:log/T-patch/zsre/zsre_T-patch_20folders_test.ckpt\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - log_name:log.txt\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - data_path:data/zsre_data\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - seed:42\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - batch_size:64\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - model_path:log/models/bart_seq2seq/version_4/checkpoints/test.ckpt\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - train_sub_size:10000\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - memory_size:40000\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - debug_mode:0\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - num_workers:0\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - example_repeat:8\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - temp_mode:0\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - get_heat_map:0\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - max_edit_step:2000\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - device:cuda:0\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - gpus:[0]\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - fold_n:0\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - early_patience:1\n",
            "2024-03-01 04:28:04,962 - __main__ - INFO - early_mode:max\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - early_thd:0.01\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - start_val_epoch:100\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - ckpt_path:log/T-patch/zsre/zsre_T-patch_20folders_test.ckpt/fold_0\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - ckpt_monitor:save_ckpt\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - ckpt_metric_mode:max\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - optim:adam\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - lr:0.01\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - weight_decay:0.01\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - lr_scheduler_factor:0.5\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - lr_scheduler_patience:1\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - use_init_weight:1\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - amplify_v:1\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - amplify_con:10.0\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - freeze_a:0\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - freeze_k:0\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - check_val_every_n_epoch:25\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - memory_loss:top1000_exp+top1000_exp\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - mlc:10.0\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - update_memory:1\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - margin_val1:3.0\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - margin_val2:-3.0\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - activate_loss:top5_exp\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - act_loss_thd:0.1\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - alc:1.0\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - act_margin_val:0.0\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - freeze_model:True\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - val_metric_type:loss\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - max_add_neuron_num:5\n",
            "2024-03-01 04:28:04,963 - __main__ - INFO - use_val:1\n",
            "Global seed set to 42\n",
            "2024-03-01 04:28:04,964 - __main__ - INFO - Loading data\n",
            "2024-03-01 04:28:12,203 - __main__ - INFO - The size of train_sub is:10000\n",
            "2024-03-01 04:28:12,203 - __main__ - INFO - The size of memory_set is:40000\n",
            "2024-03-01 04:28:12,203 - __main__ - INFO - The size of edit_test_data is:5324\n",
            "2024-03-01 04:28:12,203 - __main__ - INFO - The size of dev_data is:24051\n",
            "2024-03-01 04:28:12,203 - __main__ - INFO - The size of val_data is:15993\n",
            "2024-03-01 04:28:12,203 - __main__ - INFO - Creating editor and result class\n",
            "The cache can not be used\n",
            "2024-03-01 04:28:15,679 - __main__ - INFO - We utilize the top1000_exp+top1000_exp memory loss and construct the memory on learnt training data\n",
            "2024-03-01 04:29:29,720 - __main__ - INFO - The memories have been constructed in top1000_exp+top1000_exp method, the size of training and validation memory are [207868] and [82854]\n",
            "2024-03-01 04:29:29,738 - __main__ - INFO - The model that we edited is log/models/bart_seq2seq/version_4/checkpoints/test.ckpt\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:433: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "2024-03-01 04:31:36,519 - __main__ - INFO - The acc on test is:0.18897342681884766\n",
            "2024-03-01 04:31:36,519 - __main__ - INFO - The acc on train is:0.21400000154972076\n",
            "2024-03-01 04:31:36,519 - __main__ - INFO - The acc on edit is:0.21468821167945862\n",
            "2024-03-01 04:31:36,519 - __main__ - INFO - \n",
            "\n",
            "\n",
            "2024-03-01 04:31:36,717 - __main__ - INFO - \n",
            "\n",
            "2024-03-01 04:31:36,718 - __main__ - INFO - For this example, we add 4 neuron(s)\n",
            "2024-03-01 04:31:36,718 - __main__ - INFO - Before editing, model attains 0.0 on 5 rephrases\n",
            "2024-03-01 04:31:36,718 - __main__ - INFO - This is the 1th edit for the 1th folder\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:441: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
            "  rank_zero_deprecation(\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory /content/Transformer-Patcher/log/T-patch/zsre/zsre_T-patch_20folders_test.ckpt/fold_0 exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type     | Params\n",
            "--------------------------------------\n",
            "0 | editor   | Editor   | 278 M \n",
            "1 | edit_acc | Accuracy | 0     \n",
            "--------------------------------------\n",
            "139 M     Trainable params\n",
            "139 M     Non-trainable params\n",
            "278 M     Total params\n",
            "1,115.400 Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py:1595: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=25). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "  rank_zero_warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:85: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  warning_cache.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('save_ckpt', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  warning_cache.warn(\n",
            "This is a update and now we have 207938 train memories and 82924 val_memories\n",
            "2024-03-01 04:32:20,618 - __main__ - INFO - After editing, 1 and 0.2 edit example and its rephrases\n",
            "2024-03-01 04:32:20,619 - __main__ - INFO - We rename the model.ckpt for the new model checkpoint\n",
            "2024-03-01 04:32:20,619 - __main__ - INFO - save the historical edit data as file\n",
            "2024-03-01 04:32:20,785 - __main__ - INFO - \n",
            "\n",
            "2024-03-01 04:32:20,785 - __main__ - INFO - For this example, we add 4 neuron(s)\n",
            "2024-03-01 04:32:20,785 - __main__ - INFO - Before editing, model attains 0.0 on 5 rephrases\n",
            "2024-03-01 04:32:20,785 - __main__ - INFO - This is the 2th edit for the 1th folder\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type     | Params\n",
            "--------------------------------------\n",
            "0 | editor   | Editor   | 278 M \n",
            "1 | edit_acc | Accuracy | 0     \n",
            "--------------------------------------\n",
            "139 M     Trainable params\n",
            "139 M     Non-trainable params\n",
            "278 M     Total params\n",
            "1,115.425 Total estimated model params size (MB)\n"
          ]
        }
      ]
    }
  ]
}